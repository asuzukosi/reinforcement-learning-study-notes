{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expermiment of using dymic programming to solve arbitrary problem with policy iteration and value iteration\n",
    "import numpy as  np\n",
    "NUM_STATES = 5\n",
    "NUM_ACTIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02615745, 0.78336181, 0.40839532, 0.38801567, 0.58992588],\n",
       "       [0.72380701, 0.98730037, 0.92682389, 0.63304074, 0.46174731],\n",
       "       [0.31882491, 0.93421116, 0.86604922, 0.41009437, 0.16493008],\n",
       "       [0.04466429, 0.12080311, 0.50105751, 0.17546288, 0.72556023],\n",
       "       [0.69125179, 0.4958132 , 0.26665   , 0.41962378, 0.96816784]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sample state transition p(s_, r | s, a)\n",
    "p = np.random.uniform(0, 1, size=(NUM_STATES, NUM_STATES))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.14339917, 0.41584085, 0.34225596, 0.10932779, 0.21566567],\n",
       "        [0.45018973, 0.38749522, 0.68570135, 0.27622476, 0.28250701],\n",
       "        [0.        , 0.        , 1.        , 0.        , 0.        ],\n",
       "        [0.26025113, 0.03196472, 0.65869642, 0.63776217, 0.20665028],\n",
       "        [0.87182697, 0.09877816, 0.72491228, 0.11928014, 0.94359884]],\n",
       "\n",
       "       [[0.58050515, 0.24429769, 0.93237344, 0.06323805, 0.2080298 ],\n",
       "        [0.48762927, 0.04349498, 0.7261596 , 0.47952975, 0.94153642],\n",
       "        [0.        , 0.        , 1.        , 0.        , 0.        ],\n",
       "        [0.41564663, 0.28685354, 0.80810742, 0.78945847, 0.3005638 ],\n",
       "        [0.89786006, 0.41018395, 0.26677131, 0.9672789 , 0.08991338]],\n",
       "\n",
       "       [[0.39462617, 0.90554868, 0.30201547, 0.28636145, 0.69818032],\n",
       "        [0.72546344, 0.01739959, 0.74949358, 0.56533042, 0.78225309],\n",
       "        [0.        , 0.        , 1.        , 0.        , 0.        ],\n",
       "        [0.42835978, 0.47301234, 0.63340585, 0.17105352, 0.1391543 ],\n",
       "        [0.17193684, 0.44078753, 0.59881799, 0.96116026, 0.04975195]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this case p is a single state transition, but in decision making systems there will be multiple state transition probabilities for differnet \n",
    "# actions\n",
    "full_state_tranition = np.random.uniform(0, 1, size=(NUM_ACTIONS, NUM_STATES, NUM_STATES))\n",
    "full_state_tranition[0][2] = np.zeros(5); full_state_tranition[0][2][2] = 1\n",
    "full_state_tranition[1][2] = np.zeros(5); full_state_tranition[1][2][2] = 1\n",
    "full_state_tranition[2][2] = np.zeros(5); full_state_tranition[2][2][2] = 1\n",
    "full_state_tranition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case there is a state transition for every actions\n",
    "# now lets create the policy\n",
    "policy = np.random.randint(0, 3, size=(NUM_STATES))\n",
    "policy # this describes the action we take in each state - this is a deterministic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0028092 , 0.26338111, 0.02366215],\n",
       "       [0.03291537, 0.16001783, 0.12883582],\n",
       "       [0.13194515, 0.9690063 , 0.19958283],\n",
       "       [0.48585821, 0.45206064, 0.27754835],\n",
       "       [0.01959283, 0.90004438, 0.46959289]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochatic_policy = np.random.uniform(0, 1, size=(NUM_STATES, NUM_ACTIONS))\n",
    "stochatic_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 0, -1, -1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will desing the rewards for all the states\n",
    "rewards = [-1] * 5\n",
    "rewards[2] = 0\n",
    "rewards # all positions are zero, except the middle positions which has a reward of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_state = 2 # we are assingning our terminal state to be 2, we will be using this when experimenting on monte carlo methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6327861 , 0.00264247, 0.01055561, 0.22343428, 0.69921155])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 1, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = full_state_tranition[0][0]\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy evaluation\n",
    "def evaluate_policy(policy):\n",
    "    # randomized value for each state\n",
    "    state_value = np.random.uniform(0, 1, size=5)\n",
    "    state_value[2] = 0 # setting state value for terminal state to 0\n",
    "    max_gap = 0\n",
    "    while True:\n",
    "        for state in range(NUM_STATES):\n",
    "            action = policy[state] # returns the action to be taken\n",
    "            next_state = np.argmax(full_state_tranition[action][state]) # returns the next state based on the environment dynamics\n",
    "            reward = rewards[next_state] # gets teh reward of the next state\n",
    "            old_state = state_value[state]\n",
    "            state_value[state] = reward + discount_factor * state_value[next_state]\n",
    "            print(\"The old state value is :\", old_state, \"the new state value is :\", state_value[state])\n",
    "            current_gap = np.abs(old_state - state_value[state])\n",
    "            if current_gap > max_gap:\n",
    "                max_gap = current_gap\n",
    "        \n",
    "        print(max_gap)\n",
    "        if max_gap <= threshold:\n",
    "            break \n",
    "        else:\n",
    "            max_gap = 0\n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  0.,  0., -1.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_value_funciton = evaluate_policy(policy)\n",
    "new_value_funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(value_function):\n",
    "    # act greedily based on the value function to improve the existing policy\n",
    "    policy = np.arange(NUM_STATES)\n",
    "    for state in range(NUM_STATES):\n",
    "        action_states = [] # for each action which state will i be going to\n",
    "        for action in range(NUM_ACTIONS):\n",
    "            next_state = np.argmax(full_state_tranition[action][state])\n",
    "            value_of_next_state = value_function[next_state]\n",
    "            action_states.append(value_of_next_state) # add value of next stat to actioin states list\n",
    "        policy[state] = np.argmax(action_states) # select action whose next state value is highest\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_policy = improve_policy(new_value_funciton)\n",
    "new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(policy):\n",
    "    while True:\n",
    "        value_function = evaluate_policy(policy)\n",
    "        old_policy = policy\n",
    "        policy = improve_policy(value_function)\n",
    "        if sum(np.array(old_policy) != np.array(policy)) == 0:\n",
    "            return policy, value_function\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 1]), array([-1.,  0.,  0.,  0., -1.]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this always evaluates to the optimal policy given the environment state dynamics\n",
    "policy_iteration(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_prediction():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
