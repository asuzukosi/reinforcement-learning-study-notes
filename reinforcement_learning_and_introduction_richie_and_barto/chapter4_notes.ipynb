{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming\n",
    "# This chapter covers the use of dynamic programming to solve MDPs and bellman optimality equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interative policy evaluation\n",
    "# This is used to evaluate a particular policy by iteratively improving the value function of \n",
    "# the states till it reaches convergence at the V_pi\n",
    "\n",
    "def initialize_values(states):\n",
    "    values = dict()\n",
    "    for s in range(len(states)-1):\n",
    "        values[s] = np.random.randint(1, 5)\n",
    "    values[len(states)-1] = 0\n",
    "    \n",
    "    return values\n",
    "    \n",
    "        \n",
    "def compute_value_of_state(state, policy, state_transition, values, actions, discount):\n",
    "    \"\"\"\n",
    "    This function is used to compute the value of a particular state\n",
    "    Args:\n",
    "        state: the state we are trying to compute the value for\n",
    "        policy (_type_): the policy being followed\n",
    "        state_stransition (_type_): the state transition function\n",
    "        values (_type_): _description: the values of all the states\n",
    "        actions: all the possible actions for all the states as a map\n",
    "    \"\"\"\n",
    "    total_value = 0\n",
    "    for action in actions[state]:\n",
    "        action_prob =  policy(action, state)\n",
    "        new_state, reward = state_transition(state, action)\n",
    "        value = action_prob * (reward + discount*(values[new_state]))\n",
    "        total_value += value\n",
    "    return total_value\n",
    "\n",
    "def compute_greedy_value_of_state(state, policy, state_transition, values, actions, discount):\n",
    "    \"\"\"\n",
    "    This function is used to compute the value of a particular state\n",
    "    Args:\n",
    "        state: the state we are trying to compute the value for\n",
    "        policy (_type_): the policy being followed\n",
    "        state_stransition (_type_): the state transition function\n",
    "        values (_type_): _description: the values of all the states\n",
    "        actions: all the possible actions for all the states as a map\n",
    "    \"\"\"\n",
    "    total_value = 0\n",
    "    action =  policy(state)\n",
    "    new_state, reward = state_transition(state, action)\n",
    "    value = (reward + discount*(values[new_state]))\n",
    "    total_value += value\n",
    "    return total_value\n",
    "            \n",
    "    \n",
    "def iterative_policy_evaluation(policy, states, threshold, state_transition, actions, discount):\n",
    "    \"\"\"\n",
    "    This function iteratively adjusts the the values of all the states\n",
    "    till it reaches the actual value for all the states following a specific policy\n",
    "    Args:\n",
    "        policy (_type_): the policy we are trying to evaluate\n",
    "        states (_type_): a list of all the states\n",
    "        threshold (_type_): the point where we would be comfortable between the approximation and the true value\n",
    "        state_transitioin(_type_): how the environment adjusts its state\n",
    "    \"\"\"\n",
    "    values = initialize_values(states)\n",
    "    max_diff = 0\n",
    "    while True:\n",
    "        for state in states:\n",
    "            old_value = values[state]\n",
    "            values[state] = compute_value_of_state(state, policy, state_transition, values, actions, discount)\n",
    "            if max_diff < np.abs(old_value - values[state]):\n",
    "                max_diff = np.abs(old_value - values[state])\n",
    "            \n",
    "        \n",
    "        if max_diff <= threshold:\n",
    "            break\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(values, actions, state_transition):\n",
    "    def policy(state):\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            new_state, reward = state_transition(state, action)\n",
    "            action_values.append(reward + values[new_state])\n",
    "        # select the best action that would lead to the state of the highest value\n",
    "        return action[np.argmax(action_values)]\n",
    "        \n",
    "    # return the policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_improvement(old_policy, values, actions, states, state_transition):\n",
    "    stable = True\n",
    "    new_policy = greedy_policy(values, actions, state_transition)\n",
    "    for state in states:\n",
    "        if new_policy(state) != old_policy(state):\n",
    "            stable = False\n",
    "    return new_policy, stable\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "1\n",
      "*****\n",
      "(1,)\n",
      "2\n",
      "*****\n",
      "(2,)\n",
      "3\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "my_list = [1, 2, 3]\n",
    "\n",
    "for i, j in np.ndenumerate(my_list):\n",
    "    print(i)\n",
    "    print(j)\n",
    "    \n",
    "    print(\"*\"*5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CARS = 20\n",
    "\n",
    "actions = np.arange(-MAX_CARS, MAX_CARS+1)\n",
    "inverse_actions = {el: ind[0] for ind, el in np.ndenumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10,  -9,  -8,\n",
       "        -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,\n",
       "         6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,\n",
       "        19,  20])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-20: 0,\n",
       " -19: 1,\n",
       " -18: 2,\n",
       " -17: 3,\n",
       " -16: 4,\n",
       " -15: 5,\n",
       " -14: 6,\n",
       " -13: 7,\n",
       " -12: 8,\n",
       " -11: 9,\n",
       " -10: 10,\n",
       " -9: 11,\n",
       " -8: 12,\n",
       " -7: 13,\n",
       " -6: 14,\n",
       " -5: 15,\n",
       " -4: 16,\n",
       " -3: 17,\n",
       " -2: 18,\n",
       " -1: 19,\n",
       " 0: 20,\n",
       " 1: 21,\n",
       " 2: 22,\n",
       " 3: 23,\n",
       " 4: 24,\n",
       " 5: 25,\n",
       " 6: 26,\n",
       " 7: 27,\n",
       " 8: 28,\n",
       " 9: 29,\n",
       " 10: 30,\n",
       " 11: 31,\n",
       " 12: 32,\n",
       " 13: 33,\n",
       " 14: 34,\n",
       " 15: 35,\n",
       " 16: 36,\n",
       " 17: 37,\n",
       " 18: 38,\n",
       " 19: 39,\n",
       " 20: 40}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
