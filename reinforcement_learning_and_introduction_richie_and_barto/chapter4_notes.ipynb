{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming\n",
    "# This chapter covers the use of dynamic programming to solve MDPs and bellman optimality equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interative policy evaluation\n",
    "# This is used to evaluate a particular policy by iteratively improving the value function of \n",
    "# the states till it reaches convergence at the V_pi\n",
    "\n",
    "def initialize_values(states):\n",
    "    values = dict()\n",
    "    for s in range(len(states)-1):\n",
    "        values[s] = np.random.randint(1, 5)\n",
    "    values[len(states)-1] = 0\n",
    "    \n",
    "    return values\n",
    "    \n",
    "        \n",
    "def compute_value_of_state(state, policy, state_transition, values, actions, discount):\n",
    "    \"\"\"\n",
    "    This function is used to compute the value of a particular state\n",
    "    Args:\n",
    "        state: the state we are trying to compute the value for\n",
    "        policy (_type_): the policy being followed\n",
    "        state_stransition (_type_): the state transition function\n",
    "        values (_type_): _description: the values of all the states\n",
    "        actions: all the possible actions for all the states as a map\n",
    "    \"\"\"\n",
    "    total_value = 0\n",
    "    for action in actions[state]:\n",
    "        action_prob =  policy(action, state)\n",
    "        new_state, reward = state_transition(state, action)\n",
    "        value = action_prob * (reward + discount*(values[new_state]))\n",
    "        total_value += value\n",
    "    return total_value\n",
    "\n",
    "def compute_greedy_value_of_state(state, policy, state_transition, values, actions, discount):\n",
    "    \"\"\"\n",
    "    This function is used to compute the value of a particular state\n",
    "    Args:\n",
    "        state: the state we are trying to compute the value for\n",
    "        policy (_type_): the policy being followed\n",
    "        state_stransition (_type_): the state transition function\n",
    "        values (_type_): _description: the values of all the states\n",
    "        actions: all the possible actions for all the states as a map\n",
    "    \"\"\"\n",
    "    total_value = 0\n",
    "    action =  policy(state)\n",
    "    new_state, reward = state_transition(state, action)\n",
    "    value = (reward + discount*(values[new_state]))\n",
    "    total_value += value\n",
    "    return total_value\n",
    "            \n",
    "    \n",
    "def iterative_policy_evaluation(policy, states, threshold, state_transition, actions, discount):\n",
    "    \"\"\"\n",
    "    This function iteratively adjusts the the values of all the states\n",
    "    till it reaches the actual value for all the states following a specific policy\n",
    "    Args:\n",
    "        policy (_type_): the policy we are trying to evaluate\n",
    "        states (_type_): a list of all the states\n",
    "        threshold (_type_): the point where we would be comfortable between the approximation and the true value\n",
    "        state_transitioin(_type_): how the environment adjusts its state\n",
    "    \"\"\"\n",
    "    values = initialize_values(states)\n",
    "    max_diff = 0\n",
    "    while True:\n",
    "        for state in states:\n",
    "            old_value = values[state]\n",
    "            values[state] = compute_value_of_state(state, policy, state_transition, values, actions, discount)\n",
    "            if max_diff < np.abs(old_value - values[state]):\n",
    "                max_diff = np.abs(old_value - values[state])\n",
    "            \n",
    "        \n",
    "        if max_diff <= threshold:\n",
    "            break\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(values, actions, state_transition):\n",
    "    def policy(state):\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            new_state, reward = state_transition(state, action)\n",
    "            action_values.append(reward + values[new_state])\n",
    "        # select the best action that would lead to the state of the highest value\n",
    "        return action[np.argmax(action_values)]\n",
    "        \n",
    "    # return the policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_improvement(old_policy, values, actions, states, state_transition):\n",
    "    stable = True\n",
    "    new_policy = greedy_policy(values, actions, state_transition)\n",
    "    for state in states:\n",
    "        if new_policy(state) != old_policy(state):\n",
    "            stable = False\n",
    "    return new_policy, stable\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
